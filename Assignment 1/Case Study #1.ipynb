{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Case Study 1 : Collecting Data from Twitter\n",
    "\n",
    "** Due Date: Sep. 23, before the class**\n",
    "\n",
    "*------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TEAM Members:** Please EDIT this cell and add the names of all the team members in your team\n",
    "\n",
    "    Qian Wang\n",
    "    \n",
    "    XiaoShuai Li\n",
    "    \n",
    "    JingNan Xu\n",
    "    \n",
    "    Elizabeth K Karpinski"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Required Readings:** \n",
    "* Chapter 1 and Chapter 9 of the book [Mining the Social Web](http://www.learndatasci.com/wp-content/uploads/2015/08/Mining-the-Social-Web-2nd-Edition.pdf) \n",
    "* The codes for [Chapter 1](http://bit.ly/1qCtMrr) and [Chapter 9](http://bit.ly/1u7eP33)\n",
    "\n",
    "\n",
    "** NOTE **\n",
    "* Please don't forget to save the notebook frequently when working in IPython Notebook, otherwise the changes you made can be lost.\n",
    "\n",
    "*----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Problem 1: Sampling Twitter Data with Streaming API about a certain topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Select a topic that you are interested in, for example, \"WPI\" or \"Lady Gaga\"\n",
    "* Use Twitter Streaming API to sample a collection of tweets about this topic in real time. (It would be recommended that the number of tweets should be larger than 200, but smaller than 1 million.\n",
    "* Store the tweets you downloaded into a local file (txt file or json file) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brief Process\n",
    "1. Register the Twitter APP Account, get the consumer_key, consumer_secret, oauth_token, and oauth_token_secret.\n",
    "2. Set the search key word \"data science\", get 100 tweets per page and 50 pages with total amount is 5,000 tweets, and save the tweets data into \"tweets.json\"\n",
    "3. Seperate the text from tweets and split the text into words, then analyzed the lexicon diversity and print the top 30 words among these texts.\n",
    "4. Find the most popular tweets in your collection of tweets based on retweeted_status and retweet_count and print in pretty table.\n",
    "5. Using prettytable to plot the top 10 hashtags, top 10 user mentions that are the most popular of tweets.\n",
    "6. Used \"get_user_profile\" function to get a populor user based on \"screen_name\".\n",
    "7. Got the friends list and follower list of that popular user using twitter api.\n",
    "8. Combined friends list and follower list to find the mutual users and print them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named tweepy",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-80a2103d1a4a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtweepy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mprettytable\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPrettyTable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named tweepy"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import tweepy\n",
    "import json\n",
    "from prettytable import PrettyTable\n",
    "from collections import Counter\n",
    "import time\n",
    "from urllib2 import URLError\n",
    "from httplib import BadStatusLine\n",
    "\n",
    "#---------------------------------------------\n",
    "# Define a Function to Login Twitter API\n",
    "    # Go to http://twitter.com/apps/new to create an app and get values\n",
    "    # for these credentials that you'll need to provide in place of these\n",
    "    # empty string values that are defined as placeholders.\n",
    "    # See https://dev.twitter.com/docs/auth/oauth for more information \n",
    "    # on Twitter's OAuth implementation.\n",
    "    \n",
    "consumer_key='JWVfEBKz5BVbL2dZ1zk9qUyTM'\n",
    "consumer_secret='jui8OVKtZqHMlEdxUNbxMZljKgFTmnlqa0mIIY8TJbLD34ctWy'\n",
    "access_key='3643178415-vlWWJMVA0jMk9FohZg2jDd1lFmy1iqRyoYKKBuY'\n",
    "access_secret='N5s4rxGu1Y9zJdldG0HkkeapZfxfaEKZIV5ColusiVyaK'\n",
    "    \n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_key, access_secret)\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "class CustomStreamListener(tweepy.StreamListener):\n",
    "    def on_status(self, status):\n",
    "        print status.text\n",
    "\n",
    "    def on_error(self, status_code):\n",
    "        print >> sys.stderr, 'Encountered error with status code:', status_code\n",
    "        return True # Don't kill the stream\n",
    "\n",
    "    def on_timeout(self):\n",
    "        print >> sys.stderr, 'Timeout…'\n",
    "        return True # Don't kill the stream\n",
    "\n",
    "sapi = tweepy.streaming.Stream(auth, CustomStreamListener())\n",
    "sapi.filter(track=['curiosity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named tweepy",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-9d8dd8a91c0e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtweepy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m: No module named tweepy"
     ]
    }
   ],
   "source": [
    "import tweepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of statuses 100\n",
      "Length of statuses 200\n",
      "Length of statuses 300\n",
      "Length of statuses 400\n",
      "Length of statuses 500\n",
      "Length of statuses 600\n",
      "Length of statuses 700\n",
      "Length of statuses 800\n",
      "Length of statuses 900\n",
      "Length of statuses 1000\n",
      "Length of statuses 1100\n",
      "Length of statuses 1200\n",
      "Length of statuses 1300\n",
      "Length of statuses 1400\n",
      "Length of statuses 1500\n",
      "Length of statuses 1600\n",
      "Length of statuses 1700\n",
      "Length of statuses 1800\n",
      "Length of statuses 1900\n",
      "Length of statuses 2000\n",
      "Length of statuses 2100\n",
      "Length of statuses 2200\n",
      "Length of statuses 2300\n",
      "Length of statuses 2400\n",
      "Length of statuses 2500\n",
      "Length of statuses 2600\n",
      "Length of statuses 2700\n",
      "Length of statuses 2800\n",
      "Length of statuses 2900\n",
      "Length of statuses 3000\n",
      "Length of statuses 3100\n",
      "Length of statuses 3200\n",
      "Length of statuses 3300\n",
      "Length of statuses 3400\n",
      "Length of statuses 3500\n",
      "Length of statuses 3552\n"
     ]
    }
   ],
   "source": [
    "#----------------------------------------------\n",
    "# Your code starts here\n",
    "#   Please add comments or text cells in between to explain the general idea of each block of the code.\n",
    "#   Please feel free to add more cells below this cell if necessary\n",
    "# Import unquote to prevent url encoding errors in next_results\n",
    "from urllib import unquote\n",
    "\n",
    "# XXX: Set this variable to a trending topic, \n",
    "# or anything else for that matter. The example query below\n",
    "# was a trending topic when this content was being developed\n",
    "# and is used throughout the remainder of this chapter.\n",
    "\n",
    "q = 'ipad pro' \n",
    "\n",
    "count = 200\n",
    "\n",
    "# See https://dev.twitter.com/docs/api/1.1/get/search/tweets\n",
    "\n",
    "search_results = twitter_api.search.tweets(q=q, count=count)\n",
    "\n",
    "statuses = search_results['statuses']\n",
    "\n",
    "\n",
    "# Iterate through  more batches of results by following the cursor\n",
    "\n",
    "for _ in range(50):\n",
    "    print \"Number of statuses\", len(statuses)\n",
    "    try:\n",
    "        next_results = search_results['search_metadata']['next_results']\n",
    "    except KeyError, e: # No more results when next_results doesn't exist\n",
    "        break\n",
    "        \n",
    "    # Create a dictionary from next_results, which has the following form:\n",
    "    # ?max_id=313519052523986943&q=NCAA&include_entities=1\n",
    "    kwargs = dict([ kv.split('=') for kv in unquote(next_results[1:]).split(\"&\") ])\n",
    "    \n",
    "    search_results = twitter_api.search.tweets(**kwargs)\n",
    "    statuses += search_results['statuses']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# save the search result to json file\n",
    "\n",
    "jsondata = json.dumps(statuses, indent=4)\n",
    "\n",
    "fd = open('tweets.json', 'w')\n",
    "fd.write(jsondata)\n",
    "fd.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of statuses 100\n",
      "Number of statuses 200\n",
      "Number of statuses 300\n",
      "Number of statuses 400\n",
      "Number of statuses 500\n",
      "Number of statuses 600\n",
      "Number of statuses 700\n",
      "Number of statuses 800\n",
      "Number of statuses 900\n",
      "Number of statuses 1000\n",
      "Number of statuses 1100\n",
      "Number of statuses 1200\n",
      "Number of statuses 1300\n",
      "Number of statuses 1400\n",
      "Number of statuses 1500\n",
      "Number of statuses 1600\n",
      "Number of statuses 1700\n",
      "Number of statuses 1779\n"
     ]
    }
   ],
   "source": [
    "#----------------------------------------------\n",
    "# Your code starts here\n",
    "#   Please add comments or text cells in between to explain the general idea of each block of the code.\n",
    "#   Please feel free to add more cells below this cell if necessary\n",
    "# Import unquote to prevent url encoding errors in next_results\n",
    "from urllib import unquote\n",
    "\n",
    "# Search the surface pro\n",
    "q = 'surfacepro' \n",
    "\n",
    "count = 200\n",
    "\n",
    "# See https://dev.twitter.com/docs/api/1.1/get/search/tweets\n",
    "\n",
    "search_results = twitter_api.search.tweets(q=q, count=count)\n",
    "\n",
    "statuses_surface = search_results['statuses']\n",
    "\n",
    "\n",
    "# Iterate through  more batches of results by following the cursor\n",
    "\n",
    "for _ in range(50):\n",
    "    print \"Number of statuses\", len(statuses_surface)\n",
    "    try:\n",
    "        next_results = search_results['search_metadata']['next_results']\n",
    "    except KeyError, e: # No more results when next_results doesn't exist\n",
    "        break\n",
    "        \n",
    "    # Create a dictionary from next_results, which has the following form:\n",
    "    # ?max_id=313519052523986943&q=NCAA&include_entities=1\n",
    "    kwargs = dict([ kv.split('=') for kv in unquote(next_results[1:]).split(\"&\") ])\n",
    "    \n",
    "    search_results = twitter_api.search.tweets(**kwargs)\n",
    "    statuses_surface += search_results['statuses']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save the search result to json file\n",
    "\n",
    "jsondata = json.dumps(statuses, indent=4)\n",
    "\n",
    "fd = open('surfacepro.json', 'w')\n",
    "fd.write(jsondata)\n",
    "fd.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Problem 2: Analyzing Tweets and Tweet Entities with Frequency Analysis\n",
    "\n",
    "**1. Word Count:** \n",
    "* Use the tweets you collected in Problem 1, and compute the frequencies of the words being used in these tweets. \n",
    "* Plot a table of the top 30 words with their counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total Tweets in this collection is: 3552\n",
      "The total words in the searched tweets is: 52702\n",
      "The unique words in the searched tweets is: 10629\n",
      "The lexical diversity for tweets is: 0.202\n"
     ]
    }
   ],
   "source": [
    "#----------------------------------------------\n",
    "# Your code starts here\n",
    "#   Please add comments or text cells in between to explain the general idea of each block \n",
    "#   of the code.\n",
    "#   Please feel free to add more cells below this cell if necessary\n",
    "\n",
    "\n",
    "statuses = json.loads(open('tweets.json').read())\n",
    "print \"The total Tweets in this collection is: %d\" % len(statuses)\n",
    "\n",
    "status_texts = [status['text'] for status in statuses]\n",
    "\n",
    "# Compute a collection of all words from all tweets\n",
    "words = [ w for t in status_texts \n",
    "              for w in t.split() ]\n",
    "# the total words in all the dataset\n",
    "total = len(words)\n",
    "print \"The total words in the searched tweets is: %d\" % total\n",
    "# the unique words in the dataset\n",
    "unique = len(set(words))\n",
    "print \"The unique words in the searched tweets is: %d\" % unique\n",
    "# Calculating lexical diversity for tweets\n",
    "diversity = 1.0*unique/total\n",
    "print \"The lexical diversity for tweets is: %.3f\" % diversity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'me',\n",
       " u'my',\n",
       " u'myself',\n",
       " u'we',\n",
       " u'our',\n",
       " u'ours',\n",
       " u'ourselves',\n",
       " u'you',\n",
       " u'your']"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get English stopwords\n",
    "# get rid of the stopwords in tweets' text\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "sw = stopwords.words(\"english\")\n",
    "sw[1:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total words after remove the stopwords in the searched tweets is: 47208\n",
      "The unique words after remove the stopwords in the searched tweets is: 10527\n",
      "The lexical diversity for tweets after remove the stopwords is: 0.223\n"
     ]
    }
   ],
   "source": [
    "# remove the stopwords\n",
    "cleaned_words = [item for item in words if item not in sw]\n",
    "total = len(cleaned_words)\n",
    "print \"The total words after remove the stopwords in the searched tweets is: %d\" % total\n",
    "# the unique words in the dataset\n",
    "unique = len(set(cleaned_words))\n",
    "print \"The unique words after remove the stopwords in the searched tweets is: %d\" % unique\n",
    "# Calculating lexical diversity for tweets\n",
    "diversity = 1.0*unique/total\n",
    "print \"The lexical diversity for tweets after remove the stopwords is: %.3f\" % diversity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top 30 words with their counts:\n",
      "+-----------------+-------+\n",
      "| Word            | Count |\n",
      "+-----------------+-------+\n",
      "| iPad            |  3040 |\n",
      "| Pro             |  1735 |\n",
      "| |               |  1316 |\n",
      "| Apple           |   950 |\n",
      "| RT              |   641 |\n",
      "| Sleeve          |   433 |\n",
      "| #iPadProSleeve  |   428 |\n",
      "| #iPadProCase    |   428 |\n",
      "| #iPadProCover   |   428 |\n",
      "| -               |   412 |\n",
      "| PRO             |   401 |\n",
      "| #ipadPro        |   396 |\n",
      "| #iPadProLeather |   387 |\n",
      "| Pro,            |   377 |\n",
      "| Leather         |   303 |\n",
      "| Mini            |   295 |\n",
      "| iPhone          |   292 |\n",
      "| en              |   271 |\n",
      "| MacBook         |   270 |\n",
      "| Retina          |   252 |\n",
      "| 3,              |   244 |\n",
      "| Guide:          |   242 |\n",
      "| Discounts       |   242 |\n",
      "| Accessories     |   237 |\n",
      "| Buyer's         |   231 |\n",
      "| pro             |   189 |\n",
      "| 4               |   186 |\n",
      "| de              |   181 |\n",
      "| se              |   178 |\n",
      "| mini            |   178 |\n",
      "+-----------------+-------+\n"
     ]
    }
   ],
   "source": [
    "# Plot a table of the top 30 words with their counts\n",
    "\n",
    "cleaned_words[:10]\n",
    "c = Counter(cleaned_words)\n",
    "\n",
    "pt = PrettyTable(field_names=['Word', 'Count'])\n",
    "[ pt.add_row(kv) for kv in c.most_common()[:30] ]\n",
    "pt.align['Word'], pt.align['Count'] = 'l', 'r' # Set column alignment\n",
    "print \"The top 30 words with their counts:\"\n",
    "print pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Find the most popular tweets in your collection of tweets**\n",
    "\n",
    "Please plot a table of the top 10 tweets that are the most popular among your collection, i.e., the tweets with the largest number of retweet counts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The amount of retweeted tweets is: 612\n",
      "\n",
      "Top 10 retweets are:\n",
      "+-------+----------------+-------------------------------------------------------------------+--------------------+\n",
      "| Count | Screen Name    | Text                                                              | ID                 |\n",
      "+-------+----------------+-------------------------------------------------------------------+--------------------+\n",
      "| 2083  | ibisPaint      | RT @ibisPaint: ありがとう！アイビスペイント330万DL突破記念キャンペーン！ibisPaint公式アカウントをフォ | 645020082003771392 |\n",
      "|       |                | ロー＆このツイートをリツイートして最新機種の大きなiPad 「iPad Pro」をもらっちゃおう！詳細はこちら |                    |\n",
      "|       |                | http://t.co/0O5GLx…                                               |                    |\n",
      "| 1253  | MKBHD          | RT @MKBHD: iPad Pro pricing. Starts at $799. Pencil is $99.       | 644862668206145537 |\n",
      "|       |                | Keyboard $169. Starts in November. http://t.co/LUlTbb8vZt         |                    |\n",
      "| 1043  | anasyacharm    | RT @anasyacharm: Сегодня ночью сломала свой IPad,он упал под      | 644799731332419584 |\n",
      "|       |                | кровать и видимо что то внутри повредилось, у меня много белых    |                    |\n",
      "|       |                | полосок на дисп…                                                  |                    |\n",
      "| 1016  | MKBHD          | RT @MKBHD: This is your iPad. Now this is your iPad on steroids.  | 644831167250866176 |\n",
      "|       |                | (12.9\" iPad Pro) http://t.co/wmt2NfqQES                           |                    |\n",
      "| 921   | smartfon_kupit | RT @smartfon_kupit: Тим Кук назвал дисплей 3D Touch революцией и  | 644776562311720960 |\n",
      "|       |                | заявил, что iPad Pro может полностью заменить компьютер           |                    |\n",
      "|       |                |                                                                   |                    |\n",
      "|       |                | Тим Кук назвал  h…                                                |                    |\n",
      "| 919   | smartfon_kupit | RT @smartfon_kupit: Будущие характеристики Apple iPad Air 3       | 644880215437668353 |\n",
      "|       |                |                                                                   |                    |\n",
      "|       |                | Уже немного утихли новости про Apple iPad Pro, который появился   |                    |\n",
      "|       |                | неделю назад h…                                                   |                    |\n",
      "| 904   | hamzasood      | RT @hamzasood: Not much of a surprise but Xcode confirms 2GB of   | 644987826388512768 |\n",
      "|       |                | RAM for the 6s (and 6s plus), and 4GB for the iPad Pro            |                    |\n",
      "|       |                | http://t.co/X8Ym4Dta…                                             |                    |\n",
      "| 896   | yoware         | RT @yoware: ส่วนใครที่รอส่งอีโมจิรูปนิ้วกลาง คาดว่ายังไม่มาใน #iOS9      | 644904815592542208 |\n",
      "|       |                |                                                                   |                    |\n",
      "|       |                | ต้องรอ iOS เวอร์ชั่น 9.1 ที่ออกพร้อม iPad Pro เดือนพฤศจิกายน          |                    |\n",
      "|       |                | http://t.…                                                        |                    |\n",
      "| 884   | pokerbitchnoel | RT @pokerbitchnoel: ไอ้เหี้ย iPad pro                               | 644786019041083397 |\n",
      "|       |                | แม่งตอบโจทย์คนแต่งนิยายกับ,copy writer ปะ                           |                    |\n",
      "|       |                | แต่เงินในกระเป๋ากูไม่ตอบอะไรพวกมึงทั้งนั้น                              |                    |\n",
      "| 563   | atvechayu      | RT @atvechayu: Все эти iPhone 6S и iPad Pro конечно очень хорошо, | 644942927685402624 |\n",
      "|       |                | но черт возьми, Apple, когда ты уже решишь настоящую проблему?!   |                    |\n",
      "|       |                | http://t.…                                                        |                    |\n",
      "+-------+----------------+-------------------------------------------------------------------+--------------------+\n"
     ]
    }
   ],
   "source": [
    "#----------------------------------------------\n",
    "# Your code starts here\n",
    "#   Please add comments or text cells in between to explain the general idea of each block\n",
    "#   of the code.\n",
    "#   Please feel free to add more cells below this cell if necessary\n",
    "\n",
    "# get retweet_count, screen_name, text, and Id of the popular tweets and saved into a list\n",
    "retweets = [\n",
    "            # Store out a tuple of these three values ...\n",
    "            (status['retweet_count'], \n",
    "             status['retweeted_status']['user']['screen_name'],\n",
    "             status['text'],\n",
    "             status['id']) \n",
    "            # ... for each status ...\n",
    "            for status in statuses \n",
    "            # ... so long as the status meets this condition.\n",
    "                if status.has_key('retweeted_status')\n",
    "           ]\n",
    "print\n",
    "print \"The amount of retweeted tweets is: %d\" % len(retweets)\n",
    "print\n",
    "\n",
    "# find the top 10 popular tweets\n",
    "top = []\n",
    "n = 10\n",
    "\n",
    "for s in sorted(retweets, reverse=True):\n",
    "    if len(top) == 0:\n",
    "        top.append(s)\n",
    "    if len(top) > 0 and len(top) < n:\n",
    "        if top[-1:][0][0] != s[0]:\n",
    "            top.append(s)\n",
    "    else:\n",
    "        break\n",
    "\n",
    "print \"Top %d retweets are:\" %n\n",
    "pt = PrettyTable(field_names=['Count', 'Screen Name', 'Text','ID'])\n",
    "[ pt.add_row(row) for row in sorted(top, reverse=True) ]\n",
    "pt.max_width['Text'] = 65\n",
    "pt.align= 'l'\n",
    "print pt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Find the most popular Tweet Entities in your collection of tweets**\n",
    "\n",
    "Please plot a table of the top 10 hashtags, top 10 user mentions that are the most popular in your collection of tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Screen_names:\n",
      "+-----------------+-------+\n",
      "| Screen_name     | Count |\n",
      "+-----------------+-------+\n",
      "| ibisPaint       |    76 |\n",
      "| YouTube         |    59 |\n",
      "| smoothblink_awe |    24 |\n",
      "| smoothblink__us |    20 |\n",
      "| goetter         |    18 |\n",
      "| surface         |    17 |\n",
      "| MacRumors       |    16 |\n",
      "| PrimePropManage |    16 |\n",
      "| thurrott        |    15 |\n",
      "| necojita        |    15 |\n",
      "+-----------------+-------+\n",
      "Top 10 Hashtags:\n",
      "+----------------+-------+\n",
      "| Hashtag        | Count |\n",
      "+----------------+-------+\n",
      "| iPadProSleeve  |   428 |\n",
      "| iPadProCase    |   428 |\n",
      "| iPadProCover   |   428 |\n",
      "| ipadPro        |   396 |\n",
      "| iPadProLeather |   387 |\n",
      "| organic        |   132 |\n",
      "| iPadPro        |   113 |\n",
      "| iPad           |   104 |\n",
      "| Apple          |   102 |\n",
      "| iPadProEtui    |    79 |\n",
      "+----------------+-------+\n"
     ]
    }
   ],
   "source": [
    "# get the screen_names list\n",
    "screen_names = [ user_mention['screen_name'] \n",
    "                 for status in statuses\n",
    "                     for user_mention in status['entities']['user_mentions'] ]\n",
    "# get the hashtas list\n",
    "hashtags = [ hashtag['text'] \n",
    "             for status in statuses\n",
    "                 for hashtag in status['entities']['hashtags'] ]\n",
    "\n",
    "pop_name = Counter(screen_names)\n",
    "pop_tag = Counter(hashtags)\n",
    "    \n",
    "pt = PrettyTable(field_names=['Screen_name', 'Count'])\n",
    "[ pt.add_row(kv) for kv in pop_name.most_common()[:10] ]\n",
    "pt.align['Screen_name'], pt.align['Count'] = 'l', 'r' # Set column alignment\n",
    "print \"Top 10 Screen_names:\"\n",
    "print pt\n",
    "\n",
    "pt = PrettyTable(field_names=['Hashtag', 'Count'])\n",
    "[ pt.add_row(kv) for kv in pop_tag.most_common()[:10] ]\n",
    "pt.align['Hashtag'], pt.align['Count'] = 'l', 'r' # Set column alignment\n",
    "print \"Top 10 Hashtags:\"\n",
    "print pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*------------------------\n",
    "\n",
    "#Problem 3: Getting \"All\" friends and \"All\" followers of a popular user in twitter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* choose a popular twitter user who has many followers, such as \"ladygaga\".\n",
    "* Get the list of all friends and all followers of the twitter user.\n",
    "* Plot 20 out of the followers, plot their ID numbers and screen names in a table.\n",
    "* Plot 20 out of the friends (if the user has more than 20 friends), plot their ID numbers and screen names in a table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This function got from the book code\n",
    "\n",
    "def make_twitter_request(twitter_api_func, max_errors=10, *args, **kw): \n",
    "    \n",
    "    # A nested helper function that handles common HTTPErrors. Return an updated\n",
    "    # value for wait_period if the problem is a 500 level error. Block until the\n",
    "    # rate limit is reset if it's a rate limiting issue (429 error). Returns None\n",
    "    # for 401 and 404 errors, which requires special handling by the caller.\n",
    "    def handle_twitter_http_error(e, wait_period=2, sleep_when_rate_limited=True):\n",
    "    \n",
    "        if wait_period > 3600: # Seconds\n",
    "            print >> sys.stderr, 'Too many retries. Quitting.'\n",
    "            raise e\n",
    "    \n",
    "        # See https://dev.twitter.com/docs/error-codes-responses for common codes\n",
    "    \n",
    "        if e.e.code == 401:\n",
    "            print >> sys.stderr, 'Encountered 401 Error (Not Authorized)'\n",
    "            return None\n",
    "        elif e.e.code == 404:\n",
    "            print >> sys.stderr, 'Encountered 404 Error (Not Found)'\n",
    "            return None\n",
    "        elif e.e.code == 429: \n",
    "            print >> sys.stderr, 'Encountered 429 Error (Rate Limit Exceeded)'\n",
    "            if sleep_when_rate_limited:\n",
    "                print >> sys.stderr, \"Retrying in 15 minutes...ZzZ...\"\n",
    "                sys.stderr.flush()\n",
    "                time.sleep(60*15 + 5)\n",
    "                print >> sys.stderr, '...ZzZ...Awake now and trying again.'\n",
    "                return 2\n",
    "            else:\n",
    "                raise e # Caller must handle the rate limiting issue\n",
    "        elif e.e.code in (500, 502, 503, 504):\n",
    "            print >> sys.stderr, 'Encountered %i Error. Retrying in %i seconds' % \\\n",
    "                (e.e.code, wait_period)\n",
    "            time.sleep(wait_period)\n",
    "            wait_period *= 1.5\n",
    "            return wait_period\n",
    "        else:\n",
    "            raise e\n",
    "\n",
    "    # End of nested helper function\n",
    "    \n",
    "    wait_period = 2 \n",
    "    error_count = 0 \n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            return twitter_api_func(*args, **kw)\n",
    "        except twitter.api.TwitterHTTPError, e:\n",
    "            error_count = 0 \n",
    "            wait_period = handle_twitter_http_error(e, wait_period)\n",
    "            if wait_period is None:\n",
    "                return\n",
    "        except URLError, e:\n",
    "            error_count += 1\n",
    "            time.sleep(wait_period)\n",
    "            wait_period *= 1.5\n",
    "            print >> sys.stderr, \"URLError encountered. Continuing.\"\n",
    "            if error_count > max_errors:\n",
    "                print >> sys.stderr, \"Too many consecutive errors...bailing out.\"\n",
    "                raise\n",
    "        except BadStatusLine, e:\n",
    "            error_count += 1\n",
    "            time.sleep(wait_period)\n",
    "            wait_period *= 1.5\n",
    "            print >> sys.stderr, \"BadStatusLine encountered. Continuing.\"\n",
    "            if error_count > max_errors:\n",
    "                print >> sys.stderr, \"Too many consecutive errors...bailing out.\"\n",
    "                raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The user's friends' number is: 4179\n",
      "The user's followers' number is: 111838\n"
     ]
    }
   ],
   "source": [
    "# If return_ID_Name=False, then return the whole Tweet Object,\n",
    "# If return_ID_Name=True, then return {ID,Name} dict\n",
    "def get_user_profile(twitter_api, screen_names=None, user_ids=None, return_ID_Name=False):\n",
    "   \n",
    "    # Must have either screen_name or user_id (logical xor)\n",
    "    assert (screen_names != None) != (user_ids != None), \\\n",
    "    \"Must have screen_names or user_ids, but not both\"\n",
    "    \n",
    "    items_to_info = {}\n",
    "\n",
    "    items = screen_names or user_ids\n",
    "    \n",
    "    while len(items) > 0:\n",
    "\n",
    "        # Process 100 items at a time per the API specifications for /users/lookup.\n",
    "        # See https://dev.twitter.com/docs/api/1.1/get/users/lookup for details.\n",
    "        \n",
    "        items_str = ','.join([str(item) for item in items[:100]])\n",
    "        items = items[100:]\n",
    "\n",
    "        if screen_names:\n",
    "            response = make_twitter_request(twitter_api.users.lookup, \n",
    "                                            screen_name=items_str)\n",
    "        else: # user_ids\n",
    "            response = make_twitter_request(twitter_api.users.lookup, \n",
    "                                            user_id=items_str)\n",
    "    \n",
    "        for user_info in response:\n",
    "            if return_ID_Name:\n",
    "                items_to_info[user_info['id']] = user_info['screen_name']\n",
    "            else:\n",
    "                if screen_names:\n",
    "                    items_to_info[user_info['screen_name']] = user_info\n",
    "                else: # user_ids\n",
    "                    items_to_info[user_info['id']] = user_info\n",
    "                \n",
    "    return items_to_info\n",
    "\n",
    "# Sample usage\n",
    "screen_name = \"analyticbridge\"\n",
    "twitter_api = oauth_login()\n",
    "user = get_user_profile(twitter_api, screen_names=[screen_name]) \n",
    "\n",
    "# The user's friends' number is:\n",
    "print \"The user's friends' number is: %d\" % user[screen_name]['friends_count']\n",
    "# # The user's followers' number is:\n",
    "print \"The user's followers' number is: %d\" % user[screen_name]['followers_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1506042703135607351\n",
      "1\n",
      "1499440171628708866\n",
      "2\n",
      "1485632285609590932\n",
      "3\n",
      "1483180114517645266\n",
      "4\n",
      "1480994169844768188\n",
      "5\n",
      "1480103683349233924\n",
      "6\n",
      "1476799201804888556\n",
      "7\n",
      "1472990130644329547\n",
      "8\n",
      "1472927083699544547\n",
      "9\n",
      "1472926593431473483\n",
      "10\n",
      "1471528207052377409\n",
      "11\n",
      "1512643557157392352\n",
      "1512572673235912059\n",
      "1512499073668128198\n",
      "1512425087543357618\n",
      "1512357483911741797\n",
      "1512281350419650499\n",
      "1512209594872457657\n",
      "1512126076266787835\n",
      "1512042539244516603\n",
      "1511962362492418457\n"
     ]
    }
   ],
   "source": [
    "# Get the specific user's friends and follower list\n",
    "# Because Twitter has limitation about getting data, then I set the the limitation \n",
    "# is 3000=15*200\n",
    "\n",
    "twitter_api = oauth_login()\n",
    "friend_limit = 10\n",
    "\n",
    "# See https://dev.twitter.com/docs/api/1.1/get/friends/ids and\n",
    "# https://dev.twitter.com/docs/api/1.1/get/followers/ids for details\n",
    "# on API parameters\n",
    "\n",
    "friends_list, follower_list = [], []\n",
    "\n",
    "# get the user friends list\n",
    "i = 0\n",
    "cursor = -1\n",
    "while cursor != 0:\n",
    "    response = make_twitter_request(twitter_api.friends.list, \n",
    "                          count=200, screen_name=screen_name, cursor=cursor)\n",
    "    print response['next_cursor']\n",
    "    \n",
    "    if response is not None:\n",
    "        friends_list.append(response['users'])\n",
    "        cursor = response['next_cursor']\n",
    "        i = i + 1\n",
    "    print i\n",
    "    # if response is none or i greater than limit, then quit the loop\n",
    "    if i > friend_limit or response is None:\n",
    "        break\n",
    "\n",
    "# get the user friends list\n",
    "i = 1\n",
    "cursor = -1\n",
    "while cursor != 0:\n",
    "    response = make_twitter_request(twitter_api.followers.list, \n",
    "                          count=200, screen_name=screen_name, cursor=cursor)\n",
    "    print response['next_cursor']\n",
    "    \n",
    "    if response is not None:\n",
    "        follower_list.append(response['users'])\n",
    "        cursor = response['next_cursor']\n",
    "        i = i + 1\n",
    "    if i > friend_limit or response is None:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "friends = [n for f in friends_list \n",
    "               for n in f]\n",
    "followers = [n for f in follower_list \n",
    "               for n in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2200\n",
      "2000\n"
     ]
    }
   ],
   "source": [
    "print len(friends)\n",
    "print len(followers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'friends' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-68-d51b5876f060>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# save the search result to json file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mjsondata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfriends\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mfd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'friends.json'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mfd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjsondata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'friends' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# save the search result to json file\n",
    "\n",
    "jsondata = json.dumps(friends, indent=4)\n",
    "fd = open('friends.json', 'w')\n",
    "fd.write(jsondata)\n",
    "fd.close()\n",
    "\n",
    "jsondata = json.dumps(followers, indent=4)\n",
    "fd = open('followers.json', 'w')\n",
    "fd.write(jsondata)\n",
    "fd.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2200\n",
      "2000\n"
     ]
    }
   ],
   "source": [
    "# load the saved friends_list and follower_list\n",
    "fr_list = json.loads(open('friends.json').read())\n",
    "fo_list = json.loads(open('followers.json').read())\n",
    "print len(fr_list)\n",
    "print len(fo_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 Friends list:\n",
      "+------------+-----------------+\n",
      "| ID         | Screen Name     |\n",
      "+------------+-----------------+\n",
      "| 3434847357 | LudovicLag      |\n",
      "| 3432782331 | DataPlough      |\n",
      "| 3416176299 | DanaKha32536168 |\n",
      "| 3412588197 | DataVisi0n      |\n",
      "| 3346508445 | G_R_Maranda     |\n",
      "| 3310879135 | ROIdoctor       |\n",
      "| 3307055528 | louiskitlung    |\n",
      "| 3281655272 | GarrettLeeH     |\n",
      "| 3280000477 | ChannelioSecret |\n",
      "| 3274888813 | MSAdvAnalytics  |\n",
      "| 3257050133 | BigDataShift    |\n",
      "| 3254815362 | SociativeBigDat |\n",
      "| 3247116547 | PeterIsDown     |\n",
      "| 3245339241 | DPIllingsworth  |\n",
      "| 3241729577 | kenwilliams9000 |\n",
      "| 3228543147 | eswarvramesh2   |\n",
      "| 3225789458 | TheNutellaMan   |\n",
      "| 3225388774 | JvCode          |\n",
      "| 3223169658 | ecruzreyes      |\n",
      "| 3222850370 | AI_Scientist    |\n",
      "+------------+-----------------+\n",
      "\n",
      "20 Followers list:\n",
      "+------------+-----------------+\n",
      "| ID         | Screen Name     |\n",
      "+------------+-----------------+\n",
      "| 3701159601 | haydencod2      |\n",
      "| 3697797016 | mondobrain      |\n",
      "| 3697107509 | NicolyWoly      |\n",
      "| 3696705797 | cloudmater      |\n",
      "| 3696646996 | SocSci2030      |\n",
      "| 3695438362 | VAASharma       |\n",
      "| 3695115855 | jrcodevoyeur    |\n",
      "| 3694796295 | LekaunyaneThato |\n",
      "| 3694794321 | zhiguoais       |\n",
      "| 3694258883 | AnalyticsGurus  |\n",
      "| 3691635796 | rodrigogiungi   |\n",
      "| 3690050303 | SubhroDas11     |\n",
      "| 3689888061 | DataDoran       |\n",
      "| 3687705316 | maurysoch11     |\n",
      "| 3687395057 | ontleden        |\n",
      "| 3687285981 | nikita_karimoff |\n",
      "| 3686943615 | Berenguer4s     |\n",
      "| 3686017821 | 1hrstrategy     |\n",
      "| 3684274455 | SamLine_SAS     |\n",
      "| 3683821515 | SimonaB87       |\n",
      "+------------+-----------------+\n"
     ]
    }
   ],
   "source": [
    "# get 10 friends and 10 followers\n",
    "\n",
    "friends_id_name = [(status['id'], status['screen_name']) for status in fr_list]\n",
    "followers_id_name = [(status['id'], status['screen_name']) for status in fo_list]\n",
    "\n",
    "pt = PrettyTable(field_names=['ID', 'Screen Name'])\n",
    "[ pt.add_row(row) for row in sorted(friends_id_name, reverse=True)[:20]]\n",
    "pt.align= 'l'\n",
    "print \"20 Friends list:\"\n",
    "print pt\n",
    "print\n",
    "\n",
    "pt = PrettyTable(field_names=['ID', 'Screen Name'])\n",
    "[ pt.add_row(row) for row in sorted(followers_id_name, reverse=True)[:20]]\n",
    "pt.align= 'l'\n",
    "print \"20 Followers list:\"\n",
    "print pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Compute the mutual friends within the two groups, i.e., the users who are in both friend list and follower list, plot their ID numbers and screen names in a table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mutual friends list:\n",
      "+------------+-----------------+\n",
      "| ID         | Screen Name     |\n",
      "+------------+-----------------+\n",
      "| 2445802928 | pythontrending  |\n",
      "| 2241428113 | brytlytUK       |\n",
      "| 317005063  | AlexanderD_Beck |\n",
      "+------------+-----------------+\n"
     ]
    }
   ],
   "source": [
    "#----------------------------------------------\n",
    "# Your code starts here\n",
    "#   Please add comments or text cells in between to explain the general idea of each block of the code.\n",
    "#   Please feel free to add more cells below this cell if necessary\n",
    "\n",
    "# get the mutual friends between friends list and followers lit\n",
    "\n",
    "mutual_friends = [friend for friend in friends_id_name\n",
    "        for follower in followers_id_name\n",
    "            if friend[0] == follower[0]]\n",
    "\n",
    "pt = PrettyTable(field_names=['ID', 'Screen Name'])\n",
    "[ pt.add_row(row) for row in sorted(mutual_friends, reverse=True)[:10]]\n",
    "pt.align= 'l'\n",
    "print \"Mutual friends list:\"\n",
    "print pt\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*------------------------\n",
    "\n",
    "#Problem 4: (optional) Explore the data \n",
    "\n",
    "Run some additional experiments with your data to gain familiarity with the twitter data ant twitter API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[641809553592422400, 641709034261774336, 641644878716256260, 641515375964889088, 641737911147364354, 641668488738811905, 641632173833891841, 641631281239990272, 641629100629585920, 641629098608074752]\n",
      "Counter({u'randal_olson': 6, u'HarvardBiz': 3, u'analyticbridge': 1})\n",
      "\n",
      "Counter({u'Python': 6, u'DataScience': 6, u'MachineLearning': 6})\n"
     ]
    }
   ],
   "source": [
    "#----------------------------------------------\n",
    "# Your code starts here\n",
    "#   Please add comments or text cells in between to explain the general idea of each block of the code.\n",
    "#   Please feel free to add more cells below this cell if necessary\n",
    "\n",
    "\n",
    "# # Get the top 10 retweeted tweets id\n",
    "# l_id = []\n",
    "# for s in sorted(retweets,reverse=True)[:10]:\n",
    "#     l_id.append(s[3])\n",
    "# print l_id\n",
    "\n",
    "# # Get the top 10 tweets based on the tweets id getting from above\n",
    "# my_top10 = [status for status in statuses\n",
    "#                      for my in l_id\n",
    "#                          if status['id'] == my]\n",
    "\n",
    "# screen_names = [ user_mention['screen_name'] \n",
    "#                  for status in my_top10\n",
    "#                      for user_mention in status['entities']['user_mentions'] ]\n",
    "\n",
    "# hashtags = [ hashtag['text'] \n",
    "#              for status in my_top10\n",
    "#                  for hashtag in status['entities']['hashtags'] ]\n",
    "# print Counter(screen_names)\n",
    "# print\n",
    "# print Counter(hashtags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned words number is : 47208\n"
     ]
    }
   ],
   "source": [
    "cleaned_words = [word.lower() for word in cleaned_words]\n",
    "print \"Cleaned words number is : %d\" % len(cleaned_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load sentimental lexicon dictionary\n",
    "import pandas as pd\n",
    "\n",
    "dictionary = pd.read_table(\"dictionary.tsv\")\n",
    "words = pd.DataFrame(cleaned_words)\n",
    "words.columns = ['col3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         col1  col2         col3    col4 col5      col6\n",
      "0    weaksubj     1    abandoned     adj    n  negative\n",
      "1    weaksubj     1  abandonment    noun    n  negative\n",
      "2    weaksubj     1      abandon    verb    y  negative\n",
      "3  strongsubj     1        abase    verb    y  negative\n",
      "4  strongsubj     1    abasement  anypos    y  negative\n",
      "\n",
      "                                                col3\n",
      "0                                                 rt\n",
      "1                                        @ibispaint:\n",
      "2  ありがとう！アイビスペイント330万dl突破記念キャンペーン！ibispaint公式アカウン...\n",
      "3                                              「ipad\n",
      "4                                pro」をもらっちゃおう！詳細はこちら\n"
     ]
    }
   ],
   "source": [
    "print dictionary.head()\n",
    "print\n",
    "print words.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47895\n",
      "         col1  col2      col3    col4 col5      col6\n",
      "0  strongsubj     1    absurd     adj    n  negative\n",
      "1    weaksubj     1    accord    verb    y  positive\n",
      "2    weaksubj     1    accord    verb    y  positive\n",
      "3    weaksubj     1    accord    verb    y  positive\n",
      "4    weaksubj     1    active     adj    n  positive\n",
      "5  strongsubj     1  actually  anypos    n   neutral\n",
      "6  strongsubj     1  actually  anypos    n   neutral\n",
      "7  strongsubj     1  actually  anypos    n   neutral\n",
      "8  strongsubj     1  actually  anypos    n   neutral\n",
      "9  strongsubj     1  actually  anypos    n   neutral\n"
     ]
    }
   ],
   "source": [
    "merged = pd.merge(left=dictionary, right=words, how='right', on='col3')\n",
    "print len(merged)\n",
    "print merged.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col1</th>\n",
       "      <th>col2</th>\n",
       "      <th>col3</th>\n",
       "      <th>col4</th>\n",
       "      <th>col5</th>\n",
       "      <th>col6</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>strongsubj</td>\n",
       "      <td>1</td>\n",
       "      <td>absurd</td>\n",
       "      <td>adj</td>\n",
       "      <td>n</td>\n",
       "      <td>negative</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>weaksubj</td>\n",
       "      <td>1</td>\n",
       "      <td>accord</td>\n",
       "      <td>verb</td>\n",
       "      <td>y</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>weaksubj</td>\n",
       "      <td>1</td>\n",
       "      <td>accord</td>\n",
       "      <td>verb</td>\n",
       "      <td>y</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>weaksubj</td>\n",
       "      <td>1</td>\n",
       "      <td>accord</td>\n",
       "      <td>verb</td>\n",
       "      <td>y</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>weaksubj</td>\n",
       "      <td>1</td>\n",
       "      <td>active</td>\n",
       "      <td>adj</td>\n",
       "      <td>n</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         col1  col2    col3  col4 col5      col6  value\n",
       "0  strongsubj     1  absurd   adj    n  negative     -1\n",
       "1    weaksubj     1  accord  verb    y  positive      1\n",
       "2    weaksubj     1  accord  verb    y  positive      1\n",
       "3    weaksubj     1  accord  verb    y  positive      1\n",
       "4    weaksubj     1  active   adj    n  positive      1"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_word_value(row):\n",
    "    sentiment = row['col6']\n",
    "    if sentiment == 'negative':\n",
    "        return -1\n",
    "    elif sentiment == 'positive':\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "merged['value'] = merged.apply (lambda row: get_word_value(row),axis=1)\n",
    "\n",
    "merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The amount of positive is: 3470\n",
      "The amount of negative is: 458\n",
      "The amount of neutral is: 43967\n",
      "The amount of total words is: 47895\n",
      "The total sentimental attitude to the 'ipad pro' is : Positive\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "sum_val = sum(merged['value'])\n",
    "total_len = len(merged['value'])\n",
    "if sum_val > 0:\n",
    "    attitude = \"Positive\"\n",
    "elif sum_val < 0:\n",
    "    attitude = \"Negative\"\n",
    "else:\n",
    "    attitude = \"Neutral\"\n",
    "\n",
    "print \"The amount of positive is: %d\" % sum(merged['value'] == 1)\n",
    "print \"The amount of negative is: %d\" % sum(merged['value'] == -1)\n",
    "print \"The amount of neutral is: %d\" % sum(merged['value'] == 0)\n",
    "print \"The amount of total words is: %d\" % len(merged['value'])\n",
    "print \"The total sentimental attitude to the 'ipad pro' is : %s\" % attitude\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*-----------------\n",
    "# Done\n",
    "\n",
    "All set! \n",
    "\n",
    "** What do you need to submit?**\n",
    "\n",
    "* **Notebook File**: Save this IPython notebook, and find the notebook file in your folder (for example, \"filename.ipynb\"). This is the file you need to submit. Please make sure all the plotted tables and figures are in the notebook. If you used \"ipython notebook --pylab=inline\" to open the notebook, all the figures and tables should have shown up in the notebook.\n",
    "\n",
    "\n",
    "* **PPT Slides**: please prepare PPT slides (for 10 minutes' talk) to present about the case study . We will ask two teams which are randomly selected to present their case studies in class for this case study. \n",
    "\n",
    "* ** Report**: please prepare a report (less than 10 pages) to report what you found in the data.\n",
    "    * What data you collected? \n",
    "    * Why this topic is interesting or important to you? (Motivations)\n",
    "    * How did you analyse the data?\n",
    "    * What did you find in the data? \n",
    " \n",
    "     (please include figures or tables in the report, but no source code)\n",
    "\n",
    "Please compress all the files in a zipped file.\n",
    "\n",
    "\n",
    "** How to submit: **\n",
    "\n",
    "        Please submit through myWPI, in the Assignment \"Case Study 1\".\n",
    "        \n",
    "** Note: Each team just need to submit one submission in myWPI **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grading Criteria:\n",
    "\n",
    "** Totoal Points: 100 **\n",
    "\n",
    "\n",
    "---------------------------------------------------------------------------\n",
    "** Notebook:  **\n",
    "    Points: 60\n",
    "\n",
    "\n",
    "    -----------------------------------\n",
    "    Qestion 1:\n",
    "    Points: 20\n",
    "    -----------------------------------\n",
    "    \n",
    "    (1) Select a topic that you are interested in.\n",
    "    Points: 6 \n",
    "    \n",
    "    (2) Use Twitter Streaming API to sample a collection of tweets about this topic in real time. (It would be recommended that the number of tweets should be larger than 200, but smaller than 1 million. Please check whether the total number of tweets collected is larger than 200?\n",
    "    Points: 10 \n",
    "    \n",
    "    \n",
    "    (3) Store the tweets you downloaded into a local file (txt file or json file)\n",
    "    Points: 4 \n",
    "    \n",
    "    \n",
    "    -----------------------------------\n",
    "    Qestion 2:\n",
    "    Points: 20\n",
    "    -----------------------------------\n",
    "    \n",
    "    1. Word Count\n",
    "\n",
    "    (1) Use the tweets you collected in Problem 1, and compute the frequencies of the words being used in these tweets.\n",
    "    Points: 4 \n",
    "\n",
    "    (2) Plot a table of the top 30 words with their counts \n",
    "    Points: 4 \n",
    "    \n",
    "    2. Find the most popular tweets in your collection of tweets\n",
    "    plot a table of the top 10 tweets that are the most popular among your collection, i.e., the tweets with the largest number of retweet counts.\n",
    "    Points: 4 \n",
    "    \n",
    "    3. Find the most popular Tweet Entities in your collection of tweets\n",
    "\n",
    "    (1) plot a table of the top 10 hashtags, \n",
    "    Points: 4 \n",
    "\n",
    "    (2) top 10 user mentions that are the most popular in your collection of tweets.\n",
    "    Points: 4 \n",
    "    \n",
    "    \n",
    "    -----------------------------------\n",
    "    Qestion 3:\n",
    "    Points: 20\n",
    "    -----------------------------------\n",
    "    \n",
    "    (1) choose a popular twitter user who has many followers, such as \"ladygaga\".\n",
    "    Points: 4 \n",
    "\n",
    "    (2) Get the list of all friends and all followers of the twitter user.\n",
    "    Points: 4 \n",
    "\n",
    "    (3) Plot 20 out of the followers, plot their ID numbers and screen names in a table.\n",
    "    Points: 4 \n",
    "\n",
    "    (4) Plot 20 out of the friends (if the user has more than 20 friends), plot their ID numbers and screen names in a table.\n",
    "    Points: 4 \n",
    "    \n",
    "    (5) Compute the mutual friends within the two groups, i.e., the users who are in both friend list and follower list, plot their ID numbers and screen names in a table\n",
    "    Points: 4 \n",
    "\n",
    "    -----------------------------------\n",
    "    -----------------------------------\n",
    "    Problem 4 (Optional): Explore the data\n",
    "    Bonus points: 10\n",
    "        Novelty: 5\n",
    "        Interestingness: 5\n",
    "    -----------------------------------\n",
    "    Run some additional experiments with your data to gain familiarity with the twitter data ant twitter API\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---------------------------------------------------------------------------\n",
    "** Report: communicate the results**\n",
    "    Points: 20\n",
    "\n",
    "(1) What data you collected?\n",
    "    Points: 5 \n",
    "\n",
    "(2) Why this topic is interesting or important to you? (Motivations)\n",
    "    Points: 5 \n",
    "\n",
    "(3) How did you analyse the data?\n",
    "    Points: 5 \n",
    "\n",
    "(4) What did you find in the data?\n",
    "(please include figures or tables in the report, but no source code)\n",
    "    Points: 5 \n",
    "\n",
    "\n",
    "\n",
    "---------------------------------------------------------------------------\n",
    "** Slides (for 10 minutes of presentation): Story-telling **\n",
    "    Points: 20\n",
    "\n",
    "\n",
    "1. Motivation about the data collection, why the topic is interesting to you.\n",
    "    Points: 5 \n",
    "\n",
    "2. Communicating Results (figure/table)\n",
    "    Points: 10 \n",
    "\n",
    "3. Story telling (How all the parts (data, analysis, result) fit together as a story?)\n",
    "    Points: 5 \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
